# 小项目：名言数据分析
## 第一天
### 需要完成：
- 翻页的URL怎么来
- 用 for 循环爬多页
- 能打印出每一页抓取状态
- 程序跑完不报错
- 已知：
- BASE_URL = "https://quotes.toscrape.com/page/{}/"
- HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}
### 思考题：
- 1️⃣ BASE_URL 为什么要用 {}
  - {}为后面URL参数拼接所使用
- 2️⃣ for page in range(1, 4) 是怎么控制页数的
  - for循环从1开始到3结束左闭右开
- 3️⃣ 为什么要判断 status_code
  - status_code返回200证明请求成功，若不是200则需要注意是否被限制访问
- 4️⃣ 为什么 先 append 到 list，而不是直接写文件
  - 逐条写入的话使得IO频繁操作每次都需要打开-写入-关闭
  - 数据的一致性，为避免在爬取过程中出现的错误导致数据不完整或格式错误
  - 方便于数据的处理和清洗
  - 代码的可维护性和避免内存溢出

### 简短小记
- 学会通过 URL 规律进行翻页爬取
- 使用 for 循环批量请求页面
- 能判断请求是否成功

## 第二天
### 需要完成：
- 同时爬 名言 + 作者
- 用 dict 表示一条数据
- 用 list 保存所有数据
- 保存为 quotes.csv

### 思考题：
- 1️⃣ 为什么不用 find_all("span", class_="text") 直接爬？
  - 一个quote对应一个author，如果直接用find_all来爬取不能保证数据关联性
  - 减少数据错位风险
  - 可以获取更多相关数据
- 2️⃣ 为什么 list 里要放 dict？
  - 易于序列化和存储
  - 保持数据完整性
  - 便于数据处理和转换
  - 灵活的查询和操作
  - 更易于扩展
- 3️⃣ 为什么 CSV 是“数据分析最友好的格式”？
  - csv是纯文本格式中间用逗号隔开
  - 几乎所有的数据分析工具都支持CSV
  - 无需额外的依赖
  - 存储效率高
    - 格式；优点；缺点
    - CSV；纯文本，通用；无数据类型，无压缩
    - Excel；美观，公式；二进制，需要特定软件
    - JSON；结构化，嵌套；冗余，解析慢
    - Parquet；压缩好，列存；复杂，需要专门库
    - Pickle；Python对象；Python专用，不安全
  - 虽然CSV很友好，但也有局限性：
    - 无数据类型：所有数据都是字符串
    - 无嵌套结构：只适合二维表数据
    - 需要处理特殊字符：引号、换行、逗号等
    - 无压缩：文件体积较大
    - 无标准规范：不同方言（分隔符、引号等）

### 简短小记
- 使用 div.quote 作为数据块解析
- 将数据结构化为 list + dict
- 学会使用 csv.DictWriter 保存数据